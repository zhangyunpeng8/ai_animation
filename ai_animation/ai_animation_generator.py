# ai_animation_generator.py
"""
AIåŠ¨ç”»ç”Ÿæˆå™¨ - é›†æˆComfyUIäººç‰©ä¸€è‡´æ€§å’ŒAIé©±åŠ¨åŠ¨ç”»
"""

import asyncio
import json
import os
import sys
import time
import base64
import requests
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, field
import logging
import numpy as np
import cv2
from enum import Enum

# å¯¼å…¥ç°æœ‰çš„æ¨¡å—
try:
    from unified_comfyui_client import (
        UnifiedComfyUIClient,
        GenerationConfig,
        CharacterMethod,
        CharacterReference
    )
    from main import DubbingEngine, ProcessingConfig, CharacterProfile, VoiceStyle
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥å¿…è¦çš„æ¨¡å—")
    print("è¯·ç¡®ä¿ unified_comfyui_client.py å’Œ main.py åœ¨åŒä¸€ç›®å½•ä¸‹")
    sys.exit(1)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("AIAnimationGenerator")

class AnimationStyle(Enum):
    """åŠ¨ç”»é£æ ¼æšä¸¾"""
    ANIME = "anime"              # åŠ¨æ¼«é£æ ¼
    CINEMATIC = "cinematic"      # ç”µå½±é£æ ¼
    CARTOON = "cartoon"          # å¡é€šé£æ ¼
    REALISTIC = "realistic"      # å†™å®é£æ ¼
    PAINTERLY = "painterly"      # ç»˜ç”»é£æ ¼
    PIXEL_ART = "pixel_art"      # åƒç´ é£æ ¼

class LipSyncMethod(Enum):
    """å£å‹åŒæ­¥æ–¹æ³•æšä¸¾"""
    WHISPER_PHONEMES = "whisper_phonemes"    # WhisperéŸ³ç´ åˆ†æ
    VISEME_BASED = "viseme_based"           # è§†ä½ç´ æ˜ å°„
    DEEP_SPEECH = "deep_speech"             # æ·±åº¦è¯­éŸ³åˆ†æ
    S2P = "s2p"                             # Speech2Phoneme

class ExpressionType(Enum):
    """è¡¨æƒ…ç±»å‹æšä¸¾"""
    NEUTRAL = "neutral"          # ä¸­æ€§
    HAPPY = "happy"              # å¼€å¿ƒ
    SAD = "sad"                  # æ‚²ä¼¤
    ANGRY = "angry"              # æ„¤æ€’
    SURPRISED = "surprised"      # æƒŠè®¶
    FEARFUL = "fearful"          # å®³æ€•
    DISGUSTED = "disgusted"      # åŒæ¶
    EXCITED = "excited"          # å…´å¥‹
    THINKING = "thinking"        # æ€è€ƒ
    SPEAKING = "speaking"        # è¯´è¯

@dataclass
class AnimationConfig:
    """åŠ¨ç”»é…ç½®"""
    # åŸºç¡€é…ç½®
    resolution: tuple = (512, 768)           # åˆ†è¾¨ç‡ (å®½, é«˜)
    fps: int = 24                            # å¸§ç‡
    duration: float = 10.0                   # æ—¶é•¿(ç§’)
    style: AnimationStyle = AnimationStyle.ANIME
    background: str = ""                     # èƒŒæ™¯å›¾ç‰‡/æè¿°
    seed: int = -1                           # éšæœºç§å­
    
    # è§’è‰²é…ç½®
    character_reference: str = ""            # è§’è‰²å‚è€ƒå›¾ç‰‡
    consistency_method: CharacterMethod = CharacterMethod.IP_ADAPTER
    consistency_strength: float = 0.7        # ä¸€è‡´æ€§å¼ºåº¦
    character_scale: float = 0.8             # è§’è‰²ç¼©æ”¾
    
    # åŠ¨ç”»é…ç½®
    lip_sync_enabled: bool = True
    lip_sync_method: LipSyncMethod = LipSyncMethod.WHISPER_PHONEMES
    expression_enabled: bool = True
    head_movement_enabled: bool = True
    eye_movement_enabled: bool = True
    body_movement_enabled: bool = True
    
    # åœºæ™¯é…ç½®
    scene_description: str = ""              # åœºæ™¯æè¿°
    camera_movement: str = "subtle"          # æ‘„åƒæœºè¿åŠ¨
    lighting: str = "natural"                # å…‰ç…§
    
    # è¾“å‡ºé…ç½®
    output_format: str = "mp4"
    output_quality: str = "high"

@dataclass
class AnimationSegment:
    """åŠ¨ç”»ç‰‡æ®µ"""
    id: int
    start_time: float                        # å¼€å§‹æ—¶é—´(ç§’)
    end_time: float                          # ç»“æŸæ—¶é—´(ç§’)
    text: str                                # å°è¯æ–‡æœ¬
    character: str = "main_character"        # è¯´è¯è§’è‰²
    expression: ExpressionType = ExpressionType.SPEAKING
    audio_data: Optional[np.ndarray] = None  # éŸ³é¢‘æ•°æ®
    lip_sync_data: Optional[Dict] = None     # å£å‹åŒæ­¥æ•°æ®
    prompt: str = ""                         # åŠ¨ç”»æç¤ºè¯
    
    def __post_init__(self):
        """ååˆå§‹åŒ–"""
        self.duration = self.end_time - self.start_time

@dataclass
class CharacterModel:
    """è§’è‰²æ¨¡å‹"""
    name: str
    reference_images: List[str] = field(default_factory=list)
    voice_profile: Optional[CharacterProfile] = None
    animation_config: Dict[str, Any] = field(default_factory=dict)
    comfyui_workflow: Optional[Dict] = None
    
    def add_reference_image(self, image_path: str):
        """æ·»åŠ å‚è€ƒå›¾ç‰‡"""
        if os.path.exists(image_path):
            self.reference_images.append(image_path)
            logger.info(f"ä¸ºè§’è‰² {self.name} æ·»åŠ å‚è€ƒå›¾ç‰‡: {image_path}")
        else:
            logger.warning(f"å‚è€ƒå›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
    
    def get_comfyui_reference(self) -> CharacterReference:
        """è·å–ComfyUIå‚è€ƒå¯¹è±¡"""
        if self.reference_images:
            return CharacterReference(
                image_path=self.reference_images[0],
                name=self.name,
                face_weight=0.7,
                style_weight=0.5,
                identity_strength=0.8
            )
        else:
            raise ValueError(f"è§’è‰² {self.name} æ²¡æœ‰å‚è€ƒå›¾ç‰‡")

class AIAnimationGenerator:
    """AIåŠ¨ç”»ç”Ÿæˆå™¨ - æ ¸å¿ƒç±»"""
    
    def __init__(self, config: AnimationConfig, comfyui_host: str = "127.0.0.1", comfyui_port: int = 8188):
        self.config = config
        self.comfyui_host = comfyui_host
        self.comfyui_port = comfyui_port
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.comfyui_client = UnifiedComfyUIClient(
            host=comfyui_host,
            port=comfyui_port
        )
        
        # åˆå§‹åŒ–é…éŸ³å¼•æ“
        self.dubbing_engine = None
        
        # å­˜å‚¨æ•°æ®
        self.characters: Dict[str, CharacterModel] = {}
        self.segments: List[AnimationSegment] = []
        self.audio_segments: List[Any] = []
        self.generated_frames: List[str] = []
        
        # è¾“å‡ºç›®å½•
        self.output_dir = Path("ai_animation_output")
        self.output_dir.mkdir(exist_ok=True)
        
        logger.info("AIåŠ¨ç”»ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def initialize(self):
        """åˆå§‹åŒ–è¿æ¥"""
        logger.info("æ­£åœ¨åˆå§‹åŒ–AIåŠ¨ç”»ç”Ÿæˆå™¨...")
        
        # è¿æ¥åˆ°ComfyUI
        logger.info(f"æ­£åœ¨è¿æ¥åˆ°ComfyUI: {self.comfyui_host}:{self.comfyui_port}")
        connected = await self.comfyui_client.connect()
        
        if not connected:
            logger.error("æ— æ³•è¿æ¥åˆ°ComfyUIæœåŠ¡å™¨")
            raise ConnectionError("ComfyUIæœåŠ¡å™¨è¿æ¥å¤±è´¥")
        
        # åˆå§‹åŒ–é…éŸ³å¼•æ“
        dubbing_config = ProcessingConfig()
        self.dubbing_engine = DubbingEngine(dubbing_config)
        
        # æ£€æŸ¥å…³é”®èŠ‚ç‚¹æ˜¯å¦å¯ç”¨
        try:
            available_nodes = await self.comfyui_client.check_nodes_available()
            for node_name, is_available in available_nodes.items():
                if not is_available:
                    logger.warning(f"âš ï¸ å…³é”®èŠ‚ç‚¹ä¸å¯ç”¨: {node_name}")
        except:
            logger.warning("æ— æ³•æ£€æŸ¥èŠ‚ç‚¹å¯ç”¨æ€§ï¼Œç»§ç»­å¤„ç†...")
        
        logger.info("âœ… AIåŠ¨ç”»ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
        return True
    
    def add_character(self, character: CharacterModel):
        """æ·»åŠ è§’è‰²"""
        self.characters[character.name] = character
        logger.info(f"å·²æ·»åŠ è§’è‰²: {character.name}")
    
    def create_character_from_profile(self, profile: CharacterProfile, reference_image: str) -> CharacterModel:
        """ä»é…éŸ³è§’è‰²é…ç½®åˆ›å»ºåŠ¨ç”»è§’è‰²"""
        character = CharacterModel(
            name=profile.name,
            voice_profile=profile,
            animation_config={
                "style": self.config.style.value,
                "consistency_method": self.config.consistency_method,
                "consistency_strength": self.config.consistency_strength
            }
        )
        
        character.add_reference_image(reference_image)
        return character
    
    async def process_script(self, script_path: str):
        """å¤„ç†å‰§æœ¬"""
        logger.info(f"å¤„ç†å‰§æœ¬: {script_path}")
        
        try:
            with open(script_path, 'r', encoding='utf-8') as f:
                script_content = f.read()
            
            # è§£æå‰§æœ¬ï¼ˆç®€å•å®ç°ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„è§£æï¼‰
            lines = script_content.strip().split('\n')
            segments = []
            
            current_time = 0.0
            for i, line in enumerate(lines):
                if line.strip() and not line.startswith('#'):
                    # ç®€å•åˆ†å‰²ï¼Œå®é™…åº”è¯¥æ ¹æ®æ—¶é—´æˆ³è§£æ
                    segment_duration = 3.0  # é»˜è®¤3ç§’
                    
                    # æå–è§’è‰²å’Œå°è¯
                    parts = line.split(':', 1)
                    if len(parts) == 2:
                        character_name, dialogue = parts
                        character_name = character_name.strip()
                        dialogue = dialogue.strip()
                    else:
                        character_name = "æœªçŸ¥è§’è‰²"
                        dialogue = line.strip()
                    
                    # ç¡®å®šè§’è‰²
                    if character_name in self.characters:
                        character = character_name
                    else:
                        # ä½¿ç”¨ç¬¬ä¸€ä¸ªè§’è‰²æˆ–åˆ›å»ºé»˜è®¤è§’è‰²
                        character = list(self.characters.keys())[0] if self.characters else "main_character"
                    
                    # ç¡®å®šè¡¨æƒ…
                    expression = self._detect_expression(dialogue)
                    
                    segment = AnimationSegment(
                        id=i,
                        start_time=current_time,
                        end_time=current_time + segment_duration,
                        text=dialogue,
                        character=character,
                        expression=expression,
                        prompt=self._generate_animation_prompt(dialogue, character, expression)
                    )
                    
                    segments.append(segment)
                    current_time += segment_duration
            
            self.segments = segments
            logger.info(f"è§£æå®Œæˆ: {len(segments)}ä¸ªåŠ¨ç”»ç‰‡æ®µ")
            
            return segments
            
        except Exception as e:
            logger.error(f"å‰§æœ¬å¤„ç†å¤±è´¥: {e}")
            raise
    
    def _detect_expression(self, text: str) -> ExpressionType:
        """æ£€æµ‹è¡¨æƒ…"""
        text_lower = text.lower()
        
        # å…³é”®è¯æ£€æµ‹
        if any(word in text_lower for word in ["å“ˆå“ˆ", "å¼€å¿ƒ", "é«˜å…´", "å¿«ä¹", "laugh", "happy", "smile"]):
            return ExpressionType.HAPPY
        elif any(word in text_lower for word in ["æ‚²ä¼¤", "ä¼¤å¿ƒ", "éš¾è¿‡", "å“­", "sad", "cry", "unhappy"]):
            return ExpressionType.SAD
        elif any(word in text_lower for word in ["ç”Ÿæ°”", "æ„¤æ€’", "å‘ç«", "angry", "mad", "furious"]):
            return ExpressionType.ANGRY
        elif any(word in text_lower for word in ["æƒŠè®¶", "åƒæƒŠ", "éœ‡æƒŠ", "surprise", "shock", "amazed"]):
            return ExpressionType.SURPRISED
        elif any(word in text_lower for word in ["å®³æ€•", "ææƒ§", "ææ€–", "fear", "scared", "afraid"]):
            return ExpressionType.FEARFUL
        elif any(word in text_lower for word in ["æ¶å¿ƒ", "åŒæ¶", "è®¨åŒ", "disgust", "dislike", "hate"]):
            return ExpressionType.DISGUSTED
        elif any(word in text_lower for word in ["å…´å¥‹", "æ¿€åŠ¨", "excited", "thrilled", "energetic"]):
            return ExpressionType.EXCITED
        elif any(word in text_lower for word in ["æ€è€ƒ", "è€ƒè™‘", "æƒ³", "think", "consider", "ponder"]):
            return ExpressionType.THINKING
        else:
            return ExpressionType.SPEAKING
    
    def _generate_animation_prompt(self, dialogue: str, character: str, expression: ExpressionType) -> str:
        """ç”ŸæˆåŠ¨ç”»æç¤ºè¯"""
        character_model = self.characters.get(character)
        
        # åŸºç¡€æç¤ºè¯
        base_prompt = f"{self.config.style.value} style, "
        
        # è§’è‰²æè¿°
        if character_model and character_model.voice_profile:
            char_desc = character_model.voice_profile.name
        else:
            char_desc = character
        
        # è¡¨æƒ…æè¿°
        expression_map = {
            ExpressionType.HAPPY: "smiling happily, cheerful expression",
            ExpressionType.SAD: "sad expression, looking down, tearful eyes",
            ExpressionType.ANGRY: "angry expression, furrowed brows, clenched teeth",
            ExpressionType.SURPRISED: "surprised expression, wide eyes, open mouth",
            ExpressionType.FEARFUL: "fearful expression, scared, trembling",
            ExpressionType.DISGUSTED: "disgusted expression, wrinkled nose",
            ExpressionType.EXCITED: "excited expression, enthusiastic, energetic",
            ExpressionType.THINKING: "thinking expression, contemplative, hand on chin",
            ExpressionType.SPEAKING: "speaking, mouth open",
            ExpressionType.NEUTRAL: "neutral expression"
        }
        
        # åœºæ™¯æè¿°
        scene_desc = self.config.scene_description if self.config.scene_description else "clean background, cinematic lighting"
        
        # å®Œæ•´çš„æç¤ºè¯
        prompt = f"{base_prompt}{char_desc}, {expression_map[expression]}, {scene_desc}, "
        prompt += f"full body shot, dynamic pose, {self.config.lighting} lighting, "
        prompt += f"high quality, detailed, 4k, masterpiece"
        
        # æ·»åŠ å¯¹è¯å†…å®¹
        prompt += f", saying: \"{dialogue[:50]}\""
        
        return prompt
    
    async def generate_dubbing(self, script_path: str, output_audio_path: str):
        """ç”Ÿæˆé…éŸ³"""
        logger.info("å¼€å§‹ç”Ÿæˆé…éŸ³...")
        
        if not self.dubbing_engine:
            logger.error("é…éŸ³å¼•æ“æœªåˆå§‹åŒ–")
            return None
        
        try:
            # åˆ›å»ºä¸´æ—¶è§†é¢‘ç”¨äºé…éŸ³å¤„ç†ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨å‰§æœ¬ï¼‰
            temp_video_path = self.output_dir / "temp_scene.mp4"
            
            # ç”Ÿæˆç®€å•çš„æµ‹è¯•è§†é¢‘ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„å®ç°ï¼‰
            self._create_test_video(str(temp_video_path))
            
            # ä½¿ç”¨é…éŸ³å¼•æ“å¤„ç†
            self.dubbing_engine.process_episode_optimized(str(temp_video_path))
            
            # è¿™é‡Œåº”è¯¥è¿”å›ç”Ÿæˆçš„éŸ³é¢‘è·¯å¾„
            # å®é™…å®ç°ä¸­ï¼Œåº”è¯¥ä»dubbing_engineè·å–ç”Ÿæˆçš„éŸ³é¢‘
            
            logger.info("âœ… é…éŸ³ç”Ÿæˆå®Œæˆ")
            return output_audio_path
            
        except Exception as e:
            logger.error(f"é…éŸ³ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _create_test_video(self, output_path: str):
        """åˆ›å»ºæµ‹è¯•è§†é¢‘ï¼ˆç¤ºä¾‹ï¼‰"""
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•è§†é¢‘
        width, height = self.config.resolution
        fps = self.config.fps
        duration = 5  # 5ç§’
        
        # ä½¿ç”¨OpenCVåˆ›å»ºè§†é¢‘
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        for i in range(fps * duration):
            # åˆ›å»ºæ¸å˜èƒŒæ™¯
            frame = np.zeros((height, width, 3), dtype=np.uint8)
            
            # æ·»åŠ æ–‡æœ¬
            text = f"AI Animation Test Frame {i+1}"
            cv2.putText(frame, text, (50, height//2), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            
            out.write(frame)
        
        out.release()
        logger.info(f"åˆ›å»ºæµ‹è¯•è§†é¢‘: {output_path}")
    
    async def analyze_lip_sync(self, audio_path: str, segments: List[AnimationSegment]) -> List[Dict]:
        """åˆ†æå£å‹åŒæ­¥æ•°æ®"""
        logger.info("åˆ†æå£å‹åŒæ­¥æ•°æ®...")
        
        if not self.config.lip_sync_enabled:
            logger.info("å£å‹åŒæ­¥å·²ç¦ç”¨")
            return []
        
        lip_sync_data = []
        
        for segment in segments:
            try:
                # è¿™é‡Œåº”è¯¥ä½¿ç”¨å®é™…çš„éŸ³ç´ åˆ†æåº“
                # ç¤ºä¾‹å®ç°ï¼Œå®é™…åº”è¯¥ä½¿ç”¨whisperæˆ–ç±»ä¼¼å·¥å…·
                phoneme_data = {
                    "segment_id": segment.id,
                    "start_time": segment.start_time,
                    "end_time": segment.end_time,
                    "phonemes": self._extract_phonemes(segment.text),
                    "viseme_frames": self._generate_viseme_frames(segment)
                }
                
                segment.lip_sync_data = phoneme_data
                lip_sync_data.append(phoneme_data)
                
                logger.debug(f"åˆ†æç‰‡æ®µ {segment.id} å£å‹æ•°æ®")
                
            except Exception as e:
                logger.warning(f"ç‰‡æ®µ {segment.id} å£å‹åˆ†æå¤±è´¥: {e}")
                segment.lip_sync_data = None
        
        logger.info(f"âœ… å£å‹åŒæ­¥åˆ†æå®Œæˆ: {len(lip_sync_data)}ä¸ªç‰‡æ®µ")
        return lip_sync_data
    
    def _extract_phonemes(self, text: str) -> List[Dict]:
        """æå–éŸ³ç´ ï¼ˆç¤ºä¾‹å®ç°ï¼‰"""
        # ç®€åŒ–çš„éŸ³ç´ æ˜ å°„
        phoneme_map = {
            'a': 'AA', 'i': 'IH', 'u': 'UW', 'e': 'EH', 'o': 'AO',
            'b': 'B', 'p': 'P', 'm': 'M', 'f': 'F', 'd': 'D',
            't': 'T', 'n': 'N', 'l': 'L', 'g': 'G', 'k': 'K',
            'h': 'HH', 'j': 'Y', 'q': 'CH', 'x': 'SH', 'zh': 'ZH',
            'ch': 'CH', 'sh': 'SH', 'r': 'R', 'z': 'Z', 'c': 'TS',
            's': 'S', 'y': 'Y', 'w': 'W'
        }
        
        phonemes = []
        for char in text.lower():
            if char in phoneme_map:
                phonemes.append({
                    "phoneme": phoneme_map[char],
                    "duration": 0.1  # ç¤ºä¾‹æ—¶é•¿
                })
        
        return phonemes
    
    def _generate_viseme_frames(self, segment: AnimationSegment) -> List[Dict]:
        """ç”Ÿæˆè§†ä½ç´ å¸§ï¼ˆç¤ºä¾‹å®ç°ï¼‰"""
        frames = []
        fps = self.config.fps
        duration = segment.duration
        total_frames = int(fps * duration)
        
        for frame_idx in range(total_frames):
            frame_time = segment.start_time + (frame_idx / fps)
            
            # ç®€åŒ–çš„è§†ä½ç´ æ˜ å°„
            viseme = "rest"  # é»˜è®¤é—­åˆ
            
            # æ ¹æ®æ—¶é—´æ¨¡æ‹Ÿå£å‹å˜åŒ–
            if segment.expression == ExpressionType.SPEAKING:
                # è¯´è¯æ—¶å£å‹å˜åŒ–
                time_in_segment = frame_time - segment.start_time
                cycle = (time_in_segment * 5) % 1.0  # 5Hzå£å‹å˜åŒ–
                
                if cycle < 0.3:
                    viseme = "AA"  # å¼ å¼€
                elif cycle < 0.6:
                    viseme = "IH"  # åŠå¼ å¼€
                else:
                    viseme = "MM"  # é—­åˆ
            
            frames.append({
                "frame": frame_idx,
                "time": frame_time,
                "viseme": viseme,
                "mouth_openness": 0.5 if viseme == "AA" else 0.2
            })
        
        return frames
    
    async def generate_animation_frames(self, segments: List[AnimationSegment]) -> List[str]:
        """ç”ŸæˆåŠ¨ç”»å¸§"""
        logger.info("å¼€å§‹ç”ŸæˆåŠ¨ç”»å¸§...")
        
        generated_frames = []
        
        for segment in segments:
            logger.info(f"ç”ŸæˆåŠ¨ç”»ç‰‡æ®µ {segment.id}: {segment.text[:30]}...")
            
            try:
                # è·å–è§’è‰²ä¿¡æ¯
                character_model = self.characters.get(segment.character)
                if not character_model:
                    logger.warning(f"è§’è‰² {segment.character} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
                    character_model = next(iter(self.characters.values())) if self.characters else None
                
                # ä½¿ç”¨ComfyUIç”Ÿæˆå¸§
                if character_model and character_model.reference_images:
                    # æœ‰å‚è€ƒå›¾ç‰‡ï¼Œä½¿ç”¨äººç‰©ä¸€è‡´æ€§
                    reference_image = character_model.reference_images[0]
                    
                    # ä¸Šä¼ å‚è€ƒå›¾ç‰‡åˆ°ComfyUI
                    uploaded_name = await self.comfyui_client.upload_image(
                        Path(reference_image)
                    )
                    
                    # ç”Ÿæˆæç¤ºè¯ï¼ˆæ·»åŠ è¡¨æƒ…å’ŒåŠ¨ä½œï¼‰
                    enhanced_prompt = self._enhance_prompt_with_animation(
                        segment.prompt, segment, character_model
                    )
                    
                    # ä½¿ç”¨IP-Adapterç”Ÿæˆå›¾åƒ
                    result = await self.comfyui_client.generate_character(
                        reference_image=reference_image,
                        prompt=enhanced_prompt,
                        method=self.config.consistency_method,
                        strength=self.config.consistency_strength,
                        config=self._create_generation_config(),
                        output_dir=self.output_dir / "frames" / f"segment_{segment.id}"
                    )
                    
                    if result.get("success") and result.get("images"):
                        for img_info in result["images"]:
                            generated_frames.append({
                                "segment_id": segment.id,
                                "frame_path": img_info.get("filename", ""),
                                "time": segment.start_time,
                                "prompt": enhanced_prompt
                            })
                        
                        logger.info(f"âœ… ç‰‡æ®µ {segment.id} ç”ŸæˆæˆåŠŸ")
                    else:
                        logger.warning(f"âš ï¸ ç‰‡æ®µ {segment.id} ç”Ÿæˆå¤±è´¥: {result.get('error')}")
                
                else:
                    # æ— å‚è€ƒå›¾ç‰‡ï¼Œä½¿ç”¨æ™®é€šç”Ÿæˆ
                    result = await self.comfyui_client.generate_image(
                        prompt=segment.prompt,
                        config=self._create_generation_config(),
                        output_dir=self.output_dir / "frames" / f"segment_{segment.id}"
                    )
                    
                    if result.get("success"):
                        logger.info(f"âœ… ç‰‡æ®µ {segment.id} ç”ŸæˆæˆåŠŸ")
                    else:
                        logger.warning(f"âš ï¸ ç‰‡æ®µ {segment.id} ç”Ÿæˆå¤±è´¥")
                
            except Exception as e:
                logger.error(f"âŒ ç‰‡æ®µ {segment.id} ç”Ÿæˆå¼‚å¸¸: {e}")
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            await asyncio.sleep(1)
        
        self.generated_frames = generated_frames
        logger.info(f"âœ… åŠ¨ç”»å¸§ç”Ÿæˆå®Œæˆ: {len(generated_frames)}ä¸ªå¸§")
        return generated_frames
    
    def _enhance_prompt_with_animation(self, base_prompt: str, segment: AnimationSegment, character_model: CharacterModel) -> str:
        """å¢å¼ºæç¤ºè¯ï¼Œæ·»åŠ åŠ¨ç”»å…ƒç´ """
        enhanced = base_prompt
        
        # æ·»åŠ å£å‹ä¿¡æ¯
        if self.config.lip_sync_enabled and segment.lip_sync_data:
            # è·å–å½“å‰å¸§çš„å£å‹çŠ¶æ€
            if segment.lip_sync_data.get("viseme_frames"):
                first_frame = segment.lip_sync_data["viseme_frames"][0]
                mouth_state = "open mouth" if first_frame.get("mouth_openness", 0) > 0.4 else "closed mouth"
                enhanced += f", {mouth_state}"
        
        # æ·»åŠ è¡¨æƒ…
        expression_map = {
            ExpressionType.HAPPY: "smiling, cheerful expression",
            ExpressionType.SAD: "sad expression, tearful eyes",
            ExpressionType.ANGRY: "angry expression, furrowed brows",
            ExpressionType.SURPRISED: "surprised expression, wide eyes",
            ExpressionType.FEARFUL: "fearful expression, scared look",
            ExpressionType.DISGUSTED: "disgusted expression",
            ExpressionType.EXCITED: "excited expression, enthusiastic",
            ExpressionType.THINKING: "thinking expression, contemplative",
            ExpressionType.SPEAKING: "speaking expression",
            ExpressionType.NEUTRAL: "neutral expression"
        }
        
        enhanced += f", {expression_map.get(segment.expression, 'neutral expression')}"
        
        # æ·»åŠ å¤´éƒ¨åŠ¨ä½œ
        if self.config.head_movement_enabled:
            # ç®€å•çš„å¤´éƒ¨åŠ¨ä½œåºåˆ—
            head_motions = ["subtle head turn", "slight nod", "head tilt", "looking forward"]
            motion = head_motions[segment.id % len(head_motions)]
            enhanced += f", {motion}"
        
        # æ·»åŠ çœ¼éƒ¨åŠ¨ä½œ
        if self.config.eye_movement_enabled:
            eye_actions = ["looking at viewer", "eye contact", "blinking", "focused gaze"]
            action = eye_actions[segment.id % len(eye_actions)]
            enhanced += f", {action}"
        
        # æ·»åŠ è‚¢ä½“è¯­è¨€
        if self.config.body_movement_enabled and segment.id % 3 == 0:
            body_poses = ["hand gesture", "leaning forward", "relaxed posture", "dynamic pose"]
            pose = body_poses[segment.id % len(body_poses)]
            enhanced += f", {pose}"
        
        # æ·»åŠ æ‘„åƒæœºè§’åº¦
        camera_angles = ["medium shot", "close-up", "full body", "cinematic framing"]
        angle = camera_angles[segment.id % len(camera_angles)]
        enhanced += f", {angle}, {self.config.camera_movement} camera movement"
        
        return enhanced
    
    def _create_generation_config(self) -> GenerationConfig:
        """åˆ›å»ºç”Ÿæˆé…ç½®"""
        width, height = self.config.resolution
        
        return GenerationConfig(
            width=width,
            height=height,
            steps=25,
            cfg=7.0,
            sampler="dpmpp_2m",
            scheduler="karras",
            seed=self.config.seed if self.config.seed != -1 else -1,
            batch_size=1,
            model="sd15.safetensors",
            vae="auto"
        )
    
    async def assemble_animation(self, frames: List[Dict], audio_path: Optional[str] = None) -> str:
        """ç»„è£…åŠ¨ç”»"""
        logger.info("å¼€å§‹ç»„è£…åŠ¨ç”»...")
        
        try:
            # åˆ›å»ºè§†é¢‘è¾“å‡ºè·¯å¾„
            output_video_path = self.output_dir / f"animation_{int(time.time())}.{self.config.output_format}"
            
            # å¦‚æœæœ‰å¸§æ•°æ®ï¼Œåˆ›å»ºè§†é¢‘
            if frames:
                # æŒ‰æ—¶é—´æ’åºå¸§
                sorted_frames = sorted(frames, key=lambda x: x.get("time", 0))
                
                # åˆ›å»ºè§†é¢‘ç¼–å†™å™¨
                width, height = self.config.resolution
                fps = self.config.fps
                
                # åˆ›å»ºå¸§åºåˆ—è§†é¢‘
                frame_video_path = self.output_dir / "frame_sequence.mp4"
                self._create_frame_sequence(sorted_frames, str(frame_video_path), fps, (width, height))
                
                # å¦‚æœæœ‰éŸ³é¢‘ï¼Œåˆå¹¶éŸ³é¢‘
                if audio_path and os.path.exists(audio_path):
                    self._merge_audio_with_video(str(frame_video_path), audio_path, str(output_video_path))
                else:
                    output_video_path = frame_video_path
                
                logger.info(f"âœ… åŠ¨ç”»ç»„è£…å®Œæˆ: {output_video_path}")
                return str(output_video_path)
            else:
                logger.warning("æ²¡æœ‰å¸§æ•°æ®å¯ä¾›ç»„è£…")
                return ""
                
        except Exception as e:
            logger.error(f"åŠ¨ç”»ç»„è£…å¤±è´¥: {e}")
            return ""
    
    def _create_frame_sequence(self, frames: List[Dict], output_path: str, fps: int, resolution: tuple):
        """åˆ›å»ºå¸§åºåˆ—è§†é¢‘"""
        width, height = resolution
        
        # ä½¿ç”¨OpenCVåˆ›å»ºè§†é¢‘
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # ç¤ºä¾‹ï¼šä½¿ç”¨å ä½å›¾åƒ
        # å®é™…åº”è¯¥åŠ è½½ç”Ÿæˆçš„å¸§
        for frame_data in frames:
            # åˆ›å»ºç¤ºä¾‹å¸§ï¼ˆå®é™…åº”è¯¥åŠ è½½ç”Ÿæˆçš„å›¾åƒï¼‰
            frame = np.zeros((height, width, 3), dtype=np.uint8)
            
            # æ·»åŠ å¸§ä¿¡æ¯
            segment_id = frame_data.get("segment_id", 0)
            text = f"Frame {segment_id}"
            cv2.putText(frame, text, (50, height//2), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            
            # æ ¹æ®æ—¶é—´æˆ³é‡å¤å¸§ä»¥è¾¾åˆ°æ­£ç¡®æ—¶é•¿
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œæ¯ä¸ªå¸§é‡å¤fpsæ¬¡ï¼ˆ1ç§’ï¼‰
            for _ in range(fps):
                out.write(frame)
        
        out.release()
        logger.info(f"åˆ›å»ºå¸§åºåˆ—è§†é¢‘: {output_path}")
    
    def _merge_audio_with_video(self, video_path: str, audio_path: str, output_path: str):
        """åˆå¹¶éŸ³é¢‘å’Œè§†é¢‘"""
        try:
            import subprocess
            
            cmd = [
                'ffmpeg',
                '-i', video_path,
                '-i', audio_path,
                '-c:v', 'copy',
                '-c:a', 'aac',
                '-shortest',
                '-y',
                output_path
            ]
            
            subprocess.run(cmd, check=True, capture_output=True)
            logger.info(f"éŸ³é¢‘è§†é¢‘åˆå¹¶å®Œæˆ: {output_path}")
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘è§†é¢‘åˆå¹¶å¤±è´¥: {e}")
            # å¦‚æœå¤±è´¥ï¼Œç›´æ¥å¤åˆ¶è§†é¢‘
            import shutil
            shutil.copy(video_path, output_path)
    
    async def generate_complete_animation(self, script_path: str) -> str:
        """ç”Ÿæˆå®Œæ•´åŠ¨ç”»"""
        logger.info("å¼€å§‹ç”Ÿæˆå®Œæ•´åŠ¨ç”»...")
        
        try:
            # 1. åˆå§‹åŒ–
            await self.initialize()
            
            # 2. å¤„ç†å‰§æœ¬
            segments = await self.process_script(script_path)
            
            # 3. ç”Ÿæˆé…éŸ³
            audio_path = self.output_dir / "dubbed_audio.wav"
            await self.generate_dubbing(script_path, str(audio_path))
            
            # 4. åˆ†æå£å‹åŒæ­¥
            if self.config.lip_sync_enabled:
                await self.analyze_lip_sync(str(audio_path), segments)
            
            # 5. ç”ŸæˆåŠ¨ç”»å¸§
            frames = await self.generate_animation_frames(segments)
            
            # 6. ç»„è£…åŠ¨ç”»
            final_animation = await self.assemble_animation(frames, str(audio_path) if os.path.exists(str(audio_path)) else None)
            
            if final_animation:
                logger.info(f"ğŸ‰ åŠ¨ç”»ç”Ÿæˆå®Œæˆ: {final_animation}")
                return final_animation
            else:
                logger.error("åŠ¨ç”»ç”Ÿæˆå¤±è´¥")
                return ""
                
        except Exception as e:
            logger.error(f"åŠ¨ç”»ç”Ÿæˆè¿‡ç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return ""

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

async def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # 1. åˆ›å»ºåŠ¨ç”»é…ç½®
    animation_config = AnimationConfig(
        resolution=(512, 768),
        fps=24,
        duration=30.0,
        style=AnimationStyle.ANIME,
        consistency_method=CharacterMethod.IP_ADAPTER,
        consistency_strength=0.7,
        lip_sync_enabled=True,
        expression_enabled=True,
        scene_description="a beautiful garden with cherry blossoms",
        camera_movement="subtle pan",
        lighting="soft morning light"
    )
    
    # 2. åˆ›å»ºåŠ¨ç”»ç”Ÿæˆå™¨
    generator = AIAnimationGenerator(animation_config)
    
    # 3. åˆ›å»ºè§’è‰²
    # ä»é…éŸ³å·¥å‚çš„è§’è‰²é…ç½®åˆ›å»ºåŠ¨ç”»è§’è‰²
    voice_profile = CharacterProfile.get_preset("å“†å•¦Aæ¢¦")
    voice_profile.voice_style = VoiceStyle.LI_YUNLONG
    
    character = generator.create_character_from_profile(
        voice_profile,
        reference_image="characters/doraemon_reference.jpg"  # æ›¿æ¢ä¸ºå®é™…å›¾ç‰‡è·¯å¾„
    )
    
    generator.add_character(character)
    
    # 4. åˆ›å»ºå‰§æœ¬
    script_content = """
å“†å•¦Aæ¢¦: å¤§é›„ï¼Œä½ åˆè€ƒè¯•ä¸åŠæ ¼äº†ï¼
å¤§é›„: æˆ‘çŸ¥é“é”™äº†ï¼Œå“†å•¦Aæ¢¦...
å“†å•¦Aæ¢¦: æ‹¿å‡ºç‚¹ç”·å­æ±‰çš„æ°”æ¦‚æ¥ï¼
å¤§é›„: å¯æ˜¯æˆ‘çœŸçš„ä¸æ“…é•¿å­¦ä¹ ...
å“†å•¦Aæ¢¦: é‚£å°±è®©æˆ‘æ¥å¸®ä½ å§ï¼
"""
    
    script_path = "animation_script.txt"
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    # 5. ç”ŸæˆåŠ¨ç”»
    final_video = await generator.generate_complete_animation(script_path)
    
    if final_video:
        print(f"\nğŸ¬ åŠ¨ç”»å·²ç”Ÿæˆ: {final_video}")
        print("ğŸ‰ æ­å–œï¼AIé©±åŠ¨åŠ¨ç”»åˆ¶ä½œå®Œæˆï¼")
    else:
        print("âŒ åŠ¨ç”»ç”Ÿæˆå¤±è´¥")

# ==================== ä¸main.pyé›†æˆçš„æ¥å£ ====================

def create_animation_from_dubbing_project(dubbing_engine: DubbingEngine, output_video: str) -> str:
    """
    ä»é…éŸ³é¡¹ç›®åˆ›å»ºåŠ¨ç”»
    
    Args:
        dubbing_engine: é…éŸ³å¼•æ“å®ä¾‹
        output_video: é…éŸ³å®Œæˆçš„è§†é¢‘è·¯å¾„
    
    Returns:
        ç”Ÿæˆçš„åŠ¨ç”»è§†é¢‘è·¯å¾„
    """
    # è¿™ä¸ªå‡½æ•°å¯ä»¥åœ¨main.pyä¸­è°ƒç”¨ï¼Œå°†é…éŸ³è§†é¢‘è½¬æ¢ä¸ºAIåŠ¨ç”»
    
    # 1. æå–éŸ³é¢‘å’Œå­—å¹•
    # 2. åˆ†æå£å‹åŒæ­¥
    # 3. ç”Ÿæˆè§’è‰²åŠ¨ç”»
    # 4. åˆæˆæœ€ç»ˆè§†é¢‘
    
    # è¿”å›ç”Ÿæˆçš„åŠ¨ç”»è·¯å¾„
    return ""

# ==================== OpenAIæ¥å£é›†æˆ ====================

class OpenAIIntegration:
    """OpenAIæ¥å£é›†æˆ"""
    
    def __init__(self, api_key: str, base_url: str = "https://apis.iflow.cn/v1"):
        self.client = OpenAI(
            base_url=base_url,
            api_key=api_key
        )
        self.model = "TBStars2-200B-A13B"
    
    def generate_script_analysis(self, script: str) -> Dict:
        """ç”Ÿæˆå‰§æœ¬åˆ†æ"""
        prompt = f"""
        åˆ†æä»¥ä¸‹å‰§æœ¬ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š
        1. è§’è‰²åˆ—è¡¨
        2. æ¯ä¸ªè§’è‰²çš„å°è¯
        3. æƒ…æ„Ÿå˜åŒ–
        4. å»ºè®®çš„åŠ¨ç”»åœºæ™¯
        5. æ‘„åƒæœºè§’åº¦å»ºè®®
        
        å‰§æœ¬ï¼š
        {script}
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚
        """
        
        try:
            completion = self.client.chat.completions.create(
                extra_body={},
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )
            
            response = completion.choices[0].message.content
            
            # è§£æJSONå“åº”
            import json
            try:
                analysis = json.loads(response)
                return analysis
            except:
                # å¦‚æœå“åº”ä¸æ˜¯JSONï¼Œè¿”å›æ–‡æœ¬
                return {"raw_analysis": response}
                
        except Exception as e:
            logger.error(f"OpenAIå‰§æœ¬åˆ†æå¤±è´¥: {e}")
            return {}
    
    def enhance_animation_prompt(self, base_prompt: str, context: Dict) -> str:
        """å¢å¼ºåŠ¨ç”»æç¤ºè¯"""
        prompt = f"""
        æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œä¼˜åŒ–åŠ¨ç”»æç¤ºè¯ï¼š
        åŸºç¡€æç¤ºè¯: {base_prompt}
        ä¸Šä¸‹æ–‡ä¿¡æ¯: {json.dumps(context, ensure_ascii=False)}
        
        è¯·ç”Ÿæˆä¸€ä¸ªæ›´è¯¦ç»†ã€æ›´å…·è§†è§‰è¡¨ç°åŠ›çš„åŠ¨ç”»æç¤ºè¯ã€‚
        åŒ…å«è§’è‰²è¡¨æƒ…ã€åŠ¨ä½œã€åœºæ™¯ç»†èŠ‚ã€å…‰ç…§å’Œæ„å›¾ã€‚
        """
        
        try:
            completion = self.client.chat.completions.create(
                extra_body={},
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )
            
            enhanced_prompt = completion.choices[0].message.content
            return enhanced_prompt.strip()
            
        except Exception as e:
            logger.error(f"OpenAIæç¤ºè¯å¢å¼ºå¤±è´¥: {e}")
            return base_prompt

# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    # æ£€æŸ¥ComfyUIæœåŠ¡
    try:
        response = requests.get("http://127.0.0.1:8188", timeout=5)
        if response.status_code == 200:
            print("âœ… ComfyUIæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")
            
            # è¿è¡Œç¤ºä¾‹
            asyncio.run(example_usage())
        else:
            print(f"âš ï¸ ComfyUIæœåŠ¡å™¨å“åº”: {response.status_code}")
            print("è¯·å…ˆå¯åŠ¨ComfyUI: python main.py --port 8188")
            
    except requests.exceptions.ConnectionError:
        print("âŒ ComfyUIæœåŠ¡å™¨æœªè¿è¡Œ")
        print("è¯·å…ˆå¯åŠ¨ComfyUI: python main.py --port 8188")